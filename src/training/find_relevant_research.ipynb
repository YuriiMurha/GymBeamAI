{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0aae6366-a2ee-4140-99dd-757fcd3f4aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-27 01:39:58,277 - INFO - Using device: cpu\n",
      "2024-10-27 01:40:04,128 - INFO - Added padding token to tokenizer and resized model embeddings\n",
      "2024-10-27 01:40:04,128 - INFO - Loading and tokenizing dataset...\n",
      "2024-10-27 01:40:04,459 - INFO - Dataset tokenization complete.\n",
      "2024-10-27 01:40:04,467 - INFO - Starting training...\n",
      "2024-10-27 01:40:09,044 - INFO - Step 0 - Batch loss: 2.4602\n",
      "2024-10-27 01:40:12,982 - INFO - Step 0 - Batch loss: 3.4770\n",
      "2024-10-27 01:40:16,897 - INFO - Step 0 - Batch loss: 2.0927\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Check if a GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_name = \"EleutherAI/gpt-neo-1.3B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Add a padding token if not present\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    logger.info(\"Added padding token to tokenizer and resized model embeddings\")\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "logger.info(\"Loading and tokenizing dataset...\")\n",
    "dataset = load_dataset(\"json\", data_files=\"processed_data.jsonl\", split=\"train\")\n",
    "\n",
    "# Tokenize the dataset with metadata\n",
    "def tokenize_function(examples):\n",
    "    text_with_metadata = [\n",
    "        f\"{text} [Author: {meta.get('author', 'N/A')}] [Keywords: {meta.get('keywords', 'N/A')}]\"\n",
    "        for text, meta in zip(examples[\"text\"], examples[\"metadata\"])\n",
    "    ]\n",
    "    return tokenizer(text_with_metadata, padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "logger.info(\"Dataset tokenization complete.\")\n",
    "\n",
    "# Data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Training arguments with memory-saving and speed optimizations\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"no\",\n",
    "    learning_rate=5e-5,  # Slightly higher learning rate for faster convergence\n",
    "    per_device_train_batch_size=1,  # Small batch size for memory limitations\n",
    "    gradient_accumulation_steps=16,  # Higher accumulation for better batch simulation\n",
    "    num_train_epochs=1,  # Single epoch to minimize training time\n",
    "    weight_decay=0.01,\n",
    "    fp16=True if device == \"cuda\" else False,  # Mixed precision for faster training on GPU\n",
    "    save_steps=500,\n",
    "    save_total_limit=1,\n",
    "    logging_steps=200,  # Less frequent logging\n",
    "    warmup_steps=50,  # Lower warmup to reach full learning rate quicker\n",
    "    gradient_checkpointing=True,  # Reduces memory usage\n",
    "    report_to=[]\n",
    ")\n",
    "\n",
    "# Custom Trainer class with detailed logging\n",
    "class CustomTrainer(Trainer):\n",
    "    def training_step(self, model, inputs, *args, **kwargs):\n",
    "        model.train()\n",
    "        inputs = {k: v.to(self.args.device) for k, v in inputs.items()}\n",
    "\n",
    "        # Shift input ids and labels to the right\n",
    "        labels = inputs[\"input_ids\"].clone()\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Compute the shift in inputs for causal language modeling\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = F.cross_entropy(\n",
    "            shift_logits.view(-1, shift_logits.size(-1)),\n",
    "            shift_labels.view(-1),\n",
    "            ignore_index=tokenizer.pad_token_id\n",
    "        )\n",
    "\n",
    "        # Log batch loss\n",
    "        if self.state.global_step % 100 == 0:\n",
    "            logger.info(f\"Step {self.state.global_step} - Batch loss: {loss.item():.4f}\")\n",
    "        return loss\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Start training\n",
    "logger.info(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model_save_path = \"./fine_tuned_model\"\n",
    "logger.info(f\"Saving model and tokenizer to '{model_save_path}'\")\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "logger.info(\"Model and tokenizer saved successfully.\")\n",
    "\n",
    "# Inference function for generating responses\n",
    "def generate_response(query):\n",
    "    input_ids = tokenizer.encode(query, return_tensors=\"pt\").to(device)\n",
    "    output = model.generate(input_ids, max_length=512, num_return_sequences=1, temperature=0.7)\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Example query for testing the model\n",
    "query = \"What is the role of magnesium in pancreatic beta-cell function?\"\n",
    "response = generate_response(query)\n",
    "logger.info(f\"Generated response: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4e0ee1e-3003-4272-8f05-b1c48f77cd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the top relevant articles with citations based on your query:\n",
      "\n",
      "Title: gymbeam_hackathon_2loow3ua1h.pdf\n",
      "Author(s): Hamilton Roschel, Bruno Gualano, Sergej M. Ostojic and Eric S. Rawson\n",
      "Keywords: phosphorylcreatine; dietary supplement; cognition; brain injury; concussion\n",
      "Top Citations:\n",
      "- nutrients Review Creatine Supplementation and Brain Health Hamilton Roschel ,* , Bruno Gualano ,, Sergej .\n",
      "- Creatine Supplementation and Brain Health.\n",
      "- Ostojic and Eric .\n",
      "\n",
      "Title: gymbeam_hackathon_b7zwszxghq.pdf\n",
      "Author(s): Nuraly S. Akimbekov\n",
      "Keywords: magnesium; diabetes; insulin; glucose; β-cells\n",
      "Top Citations:\n",
      "- Most magnesium is present in bone and teeth (∼%), while its concentrations in intracellular compartments and extracellular ﬂuids constitute ∼% and <%, respectively (, –).\n",
      "- Razzaque mohammed.razzaque@uyrgv.edu; msr.nagasaki@gmail.com  July 2 September 2 September 2 Akimbekov , Coban , AtﬁA and Razzaque  (2) The role of magnesium in pancreatic beta-cell function and homeostasis.\n",
      "- The role of magnesium in pancreatic beta-cell function and homeostasis Nuraly .\n",
      "\n",
      "Title: gymbeam_hackathon_ynz2zh4k65.pdf\n",
      "Author(s): Ha-Rim Kim, Seung-Hyeon Lee, Eun-Mi Noh, Bongsuk Choi, Hyang-Yim Seo, Hansu Jang, Seon-Young Kim and Mi Hee Park\n",
      "Keywords: enzyme hydrolyzation; cervi cornu collagen; NP-2007; osteoarthritis; monosodium iodoacetate\n",
      "Top Citations:\n",
      "- Therapeutic Effect of Enzymatically Hydrolyzed Cervi Cornu Collagen -2and Potential for Application in Osteoarthritis Treatment.\n",
      "\n",
      "\n",
      "Generated response:\n",
      "What is magnesium and its role in health?\n",
      "\n",
      "Manganese (Mg) is an important metal in the Earth's crust. It is a metal with many different uses, including metal, oil, metals, and metals, which are used as energy sources for the Earth's energy supply. It is also used to reduce CO2 emissions from the atmosphere. The concentration of Mg in the Earth's atmosphere is a key factor that controls the amount of CO2 released from the atmosphere.\n",
      "\n",
      "Manganese is an important metal for many uses in the Earth's crust, including building up minerals, reducing carbon dioxide emissions, and building up carbon dioxide. Mg is also used in a variety of other processes, such as removing waste from\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model_path = \"./fine_tuned_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "# Set device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Load the dataset for retrieving article information\n",
    "dataset = load_dataset(\"json\", data_files=\"processed_data.jsonl\", split=\"train\")\n",
    "\n",
    "# Function to clean and tokenize text for better matching\n",
    "def preprocess_text(text):\n",
    "    return re.sub(r'\\W+', ' ', text.lower())\n",
    "\n",
    "# Function to rank and retrieve the top 3 relevant citations based on query\n",
    "def find_relevant_articles(query, dataset, max_results=3):\n",
    "    query_tokens = set(preprocess_text(query).split())\n",
    "    relevant_articles = []\n",
    "    \n",
    "    for article in dataset:\n",
    "        article_text = article[\"text\"]\n",
    "        sentences = re.split(r'(?<=[.!?]) +', article_text)  # Split text into sentences\n",
    "        \n",
    "        # Score each sentence based on keyword overlap\n",
    "        sentence_scores = []\n",
    "        for sentence in sentences:\n",
    "            sentence_tokens = set(preprocess_text(sentence).split())\n",
    "            overlap = query_tokens.intersection(sentence_tokens)\n",
    "            score = len(overlap)  # Simple score based on number of overlapping keywords\n",
    "            if score > 0:\n",
    "                sentence_scores.append((score, sentence))\n",
    "        \n",
    "        # Sort sentences by score in descending order and take top 3\n",
    "        top_citations = [sent for _, sent in sorted(sentence_scores, key=lambda x: x[0], reverse=True)[:3]]\n",
    "        \n",
    "        # If relevant citations are found, add them to the list\n",
    "        if top_citations:\n",
    "            relevant_articles.append({\n",
    "                \"title\": article[\"name\"],\n",
    "                \"author\": article[\"metadata\"].get(\"author\", \"N/A\"),\n",
    "                \"keywords\": article[\"metadata\"].get(\"keywords\", \"N/A\"),\n",
    "                \"citations\": top_citations\n",
    "            })\n",
    "            \n",
    "        if len(relevant_articles) >= max_results:\n",
    "            break\n",
    "    \n",
    "    return relevant_articles\n",
    "\n",
    "# Function for generating text based on the query\n",
    "def generate_response(query, max_length=150, temperature=0.7, top_p=0.9):\n",
    "    # Tokenize the query\n",
    "    input_ids = tokenizer.encode(query, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate a response from the model\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            num_return_sequences=1\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Combined function to get relevant articles with top citations and generate a response\n",
    "def query_model_with_articles(query):\n",
    "    articles = find_relevant_articles(query, dataset)\n",
    "    \n",
    "    if articles:\n",
    "        article_response = \"Here are the top relevant articles with citations based on your query:\\n\\n\"\n",
    "        for article in articles:\n",
    "            article_response += (\n",
    "                f\"Title: {article['title']}\\n\"\n",
    "                f\"Author(s): {article['author']}\\n\"\n",
    "                f\"Keywords: {article['keywords']}\\n\"\n",
    "                \"Top Citations:\\n\"\n",
    "            )\n",
    "            for citation in article[\"citations\"]:\n",
    "                article_response += f\"- {citation}\\n\"\n",
    "            article_response += \"\\n\"\n",
    "    else:\n",
    "        article_response = \"No relevant articles found in the dataset.\"\n",
    "\n",
    "    # Generate additional context from the model\n",
    "    model_response = generate_response(query)\n",
    "    return f\"{article_response}\\nGenerated response:\\n{model_response}\"\n",
    "\n",
    "# Example usage\n",
    "query = \"What is magnesium and its role in health?\"\n",
    "response = query_model_with_articles(query)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeecca80-7e16-41a8-9423-ba4ce2321004",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
